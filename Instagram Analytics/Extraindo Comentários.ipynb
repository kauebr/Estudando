{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://i.ibb.co/1T5Wbn6/Banner-para-Linkedin-capa-de-perfil-para-programador.png\"  alt=\"KCode Banner\"  />\n",
    "\n",
    "<a href=\"https://www.linkedin.com/in/kauebr/\"><img src=\"https://cdn-icons-png.flaticon.com/512/174/174857.png\" width=\"30\" ></a> <a href=\"mailto: kauemandarino@gmail.com\"><img src=\"https://cdn-icons-png.flaticon.com/512/5968/5968534.png\" width=\"30\" ></a><a href=\"\n",
    "https://api.whatsapp.com/send?phone=5541987947490&text=Oi\"> <img src=\"https://imagepng.org/wp-content/uploads/2017/08/whatsapp-icone-1.png\" width=\"30\" ></a><a href=\"https://github.com/kauebr/\"><img src=\"https://github.githubassets.com/images/modules/logos_page/Octocat.png\" width=\"30\" ></a>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temas Quentes Brasil \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objetivo:\n",
    "\n",
    "* Hoje vou criar um identificador de temas quentes em sites de notícias. O objetivo é usar web scraping para acessar sites de noticias brasileiros e retornar as palavras mais frequentes, desconsiderando termos de coesão e conectivos como \"não\", \"de\" e \"o\".\n",
    "Os sites utilizados serão: BBC Brasil, CNN Brasil, Correio Braziliense, Folha de São Paulo, Brasil 247 e Sputnik Brasil."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metodologia/Passos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Importar a biblioteca urllib.request para acessar uma URL;\n",
    "* Utilizar a biblioteca BeautifulSoup para processar o HTML e transformá-lo em texto;\n",
    "* Utilizar a biblioteca re para remover pontuação e espaços em branco do texto;\n",
    "* Criar uma lista de termos a serem desconsiderados;\n",
    "* Importar a biblioteca collections para contar as palavras com baixo processamento;\n",
    "* Utilizar um loop for para listar as palavras que aparecem mais de três vezes;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brasil: 50\n",
      "Lula: 23\n",
      "EUA: 21\n",
      "anos: 20\n",
      "ministro: 13\n",
      "pessoas: 12\n",
      "Portugal: 10\n",
      "ex: 10\n",
      "GSI: 10\n",
      "China: 9\n",
      "contra: 9\n",
      "revela: 9\n",
      "governo: 8\n",
      "dia: 8\n",
      "militares: 8\n",
      "DF: 7\n",
      "militar: 7\n",
      "guerra: 6\n",
      "Viagem: 6\n",
      "domingo: 6\n",
      "US: 6\n",
      "conflito: 5\n",
      "vida: 5\n",
      "Campos: 5\n",
      "visita: 5\n",
      "Pacheco: 5\n",
      "STF: 5\n",
      "foguete: 5\n",
      "armas: 5\n",
      "fala: 5\n",
      "novo: 5\n",
      "APRESENTADO: 5\n",
      "fronteiras: 5\n",
      "riscos: 4\n",
      "principais: 4\n",
      "Lua: 4\n",
      "muito: 4\n",
      "feira: 4\n",
      "especialistas: 4\n",
      "recursos: 4\n",
      "juros: 4\n",
      "Banco: 4\n",
      "Celso: 4\n",
      "Amorim: 4\n",
      "Silva: 4\n",
      "viagem: 4\n",
      "Planalto: 4\n",
      "morte: 4\n",
      "russo: 4\n",
      "Elon: 4\n",
      "Musk: 4\n",
      "teste: 4\n",
      "SpaceX: 4\n",
      "espera: 4\n",
      "Norte: 4\n",
      "suas: 4\n",
      "podem: 4\n",
      "E: 4\n",
      "Paulo: 4\n",
      "muda: 4\n",
      "podcast: 4\n",
      "Nacional: 4\n",
      "Estilo: 4\n",
      "Equipe: 4\n",
      "transtornos: 4\n",
      "mentais: 4\n",
      "Partida: 4\n",
      "encerrada: 4\n",
      "NACIONAL: 4\n",
      "torna: 4\n",
      "anuncia: 4\n",
      "luxo: 4\n",
      "alto: 4\n",
      "Federal: 4\n",
      "luta: 4\n",
      "fazer: 4\n",
      "Confira: 4\n",
      "Bolsonaro: 4\n",
      "fim: 4\n",
      "EM: 4\n",
      "TV: 4\n",
      "Revista: 4\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Define as URLs a serem lidas\n",
    "urls = ['https://www.bbc.com/portuguese', 'https://www.cnnbrasil.com.br/', 'https://www.correiobraziliense.com.br/', 'https://br.sputniknews.com/']\n",
    "\n",
    "# Cria uma lista para armazenar as palavras de todos os sites\n",
    "\n",
    "palavras_totais = []\n",
    "\n",
    "# Itera sobre as URLs\n",
    "for url in urls:\n",
    "    # Faz a requisição e armazena o conteúdo na variável 'html'\n",
    "    with urllib.request.urlopen(url) as response:\n",
    "        html = response.read()\n",
    "\n",
    "    # Cria um objeto BeautifulSoup para processar o HTML\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Extrai o texto do HTML e remove as tags\n",
    "    texto = soup.get_text()\n",
    "\n",
    "    # Remove números, pontuação e espaços em branco\n",
    "    palavras = re.findall(r'\\b[a-zA-Z]+\\b', texto)\n",
    "\n",
    "    # Adiciona palavras ignoradas\n",
    "    ignoradas = [\n",
    "    'não', 'no', 'e', 'na', 'para', 'de', 'diz', 'que', 'A', 'com', 'do', 'dos',\n",
    "    'sobre', 'mais', 'o', 'em', 'a', 'se', 'da', 'por', 'CNN', 'abril', 'as',\n",
    "    'uma', 'como', 'um', 'os', 'ao', 'Correio', 'pode', 'das', 'seu', 'pelo',\n",
    "    'foi', 'News', 'presidente', 'nos', 'especial', 'mas', 'ter', 'mundo', 'BBC',\n",
    "    'ser', 'sua', 'neste', 'Esportes', 'CB', 'entre', 'imagens', 'tem', 'Economia',\n",
    "    'INTERNACIONAL', 'sem', 'nem', 'aos', 'Ao', 'Dias', 'Gastronomia', 'Tecnologia',\n",
    "    'pede', 'veja', 'Braziliense', 'maior', 'quando', 'tempo', 'mesmo', 'crise: 5',\n",
    "    'vai', 'nesta', 'ele', 'deve', 'cidade', 'Duration', 'Internacional', 'Mundo',\n",
    "    'volta', 'O', 'Mais', 'POR', 'Vrum', 'Sputnik', 'faz', 'estar', 'ou', 'era',\n",
    "    'Uma', 'nas', 'FOTOS','Ex', 'pela', 'todos', 'querem', 'da','R'\n",
    "    ]\n",
    "\n",
    "    # Adiciona as palavras à lista de palavras totais\n",
    "    palavras_totais += [palavra for palavra in palavras if palavra not in ignoradas]\n",
    "\n",
    "# Usa o Counter para contar as ocorrências de cada palavra\n",
    "contagem = Counter(palavras_totais)\n",
    "\n",
    "# Imprime as palavras que aparecem mais de 3 vezes em ordem decrescente de frequência\n",
    "for palavra, freq in contagem.most_common():\n",
    "    if freq > 3:\n",
    "        print(f'{palavra}: {freq}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d11380547bba1e6c4b0ab7b7614350493a6d4caf256d9be4e363829d8f5c452"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
